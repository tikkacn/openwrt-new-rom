name: 检查CloudFront缓存结构及空间利用（仿主编译脚本）

on:
  workflow_dispatch:

env:
  CF_CDN_DOMAIN: https://d16xdi3lv2va77.cloudfront.net
  S3_EFFECTIVE_PATH_PREFIX: openwrt-s3chunks-v3/master # 如有分支或自定义路径请更改
  S3_DL_DIR_ARCHIVE_BASENAME: openwrt_dl_cache.tar.zst
  S3_STAGING_DIR_ARCHIVE_BASENAME: openwrt_staging_dir_cache.tar.zst
  S3_BUILD_DIR_HOST_ARCHIVE_BASENAME: openwrt_build_dir_host_cache.tar.zst
  S3_BUILD_DIR_TOOLCHAIN_ARCHIVE_BASENAME: openwrt_build_dir_toolchain_cache.tar.zst
  S3_BUILD_DIR_TARGET_ARCHIVE_BASENAME: openwrt_build_dir_target.tar.zst
  S3_FEEDS_CACHE_ARCHIVE_BASENAME: openwrt_feeds_cache.tar.zst
  S3_CCACHE_ARCHIVE_BASENAME: openwrt_ccache.tar.zst
  S3_BUILD_STATE_ARCHIVE_BASENAME: openwrt_build_state.tar.zst

jobs:
  check_cache:
    runs-on: ubuntu-22.04
    steps:
      - name: 最大化Runner空间
        uses: easimon/maximize-build-space@master
        with:
          root-reserve-mb: 20480
          swap-size-mb: 4096
          remove-dotnet: 'true'
          remove-android: 'true'
          remove-haskell: 'true'
          remove-codeql: 'true'
          remove-docker-images: 'false'
          build-mount-path: '/workdir'

      - name: Space check (before restore)
        run: |
          echo "== RUNNER DISK USAGE (BEFORE) =="
          df -h
          echo "== /workdir USAGE (BEFORE) =="
          du -sh /workdir/* || true

      - name: 还原CloudFront缓存到临时目录
        env:
          CF_EFFECTIVE_URL_PREFIX: ${{ env.CF_CDN_DOMAIN }}/${{ env.S3_EFFECTIVE_PATH_PREFIX }}
        run: |
          set -e
          RESTORE_TMP=/workdir/cloudfront_cache_check_tmp
          mkdir -p "$RESTORE_TMP"

          cf_download_and_extract_parts() {
            local archive_basename="$1"
            local target_extract_path="$2"
            local chunk_restore_tmp_dir="/tmp/cf_restore_chunks_$(echo "$archive_basename" | tr -dc 'a-zA-Z0-9_')"
            mkdir -p "${chunk_restore_tmp_dir}"
            local manifest_url="${CF_EFFECTIVE_URL_PREFIX}/${archive_basename}.manifest"
            local local_manifest_file="${chunk_restore_tmp_dir}/${archive_basename}.manifest"
            if curl -fsSL --retry 3 -o "${local_manifest_file}" "${manifest_url}"; then
              mapfile -t chunk_files_to_download < "${local_manifest_file}"
              if [ ${#chunk_files_to_download[@]} -eq 0 ]; then rm -rf "${chunk_restore_tmp_dir}"; return; fi
              local all_chunks_downloaded=true; declare -a downloaded_chunk_paths_ordered
              for chunk_filename_dirty in "${chunk_files_to_download[@]}"; do
                local chunk_filename=$(echo "$chunk_filename_dirty" | tr -d '\r\n'); [ -z "$chunk_filename" ] && continue
                local chunk_url="${CF_EFFECTIVE_URL_PREFIX}/${chunk_filename}"; local local_chunk_path="${chunk_restore_tmp_dir}/${chunk_filename}"
                if ! curl -fSL --retry 3 -o "${local_chunk_path}" "${chunk_url}"; then all_chunks_downloaded=false; fi
                downloaded_chunk_paths_ordered+=("${local_chunk_path}")
              done
              if [ "$all_chunks_downloaded" = true ] && [ ${#downloaded_chunk_paths_ordered[@]} -gt 0 ]; then
                cat ${downloaded_chunk_paths_ordered[@]} | zstd -d -T0 - | tar -xf - -C "${target_extract_path}"
              fi
            else
              local single_archive_url="${CF_EFFECTIVE_URL_PREFIX}/${archive_basename}"
              local single_local_archive="${chunk_restore_tmp_dir}/${archive_basename}"
              if curl -fSL --retry 3 -o "${single_local_archive}" "${single_archive_url}"; then
                tar -I "zstd -T0" -xf "${single_local_archive}" -C "${target_extract_path}"
              fi
            fi
            rm -rf "${chunk_restore_tmp_dir}"
          }
          cf_download_and_extract_parts "${{ env.S3_DL_DIR_ARCHIVE_BASENAME }}" "$RESTORE_TMP"
          cf_download_and_extract_parts "${{ env.S3_STAGING_DIR_ARCHIVE_BASENAME }}" "$RESTORE_TMP"
          cf_download_and_extract_parts "${{ env.S3_BUILD_DIR_HOST_ARCHIVE_BASENAME }}" "$RESTORE_TMP"
          cf_download_and_extract_parts "${{ env.S3_BUILD_DIR_TOOLCHAIN_ARCHIVE_BASENAME }}" "$RESTORE_TMP"
          cf_download_and_extract_parts "${{ env.S3_BUILD_DIR_TARGET_ARCHIVE_BASENAME }}" "$RESTORE_TMP"
          cf_download_and_extract_parts "${{ env.S3_FEEDS_CACHE_ARCHIVE_BASENAME }}" "$RESTORE_TMP"
          cf_download_and_extract_parts "${{ env.S3_CCACHE_ARCHIVE_BASENAME }}" "$RESTORE_TMP"
          cf_download_and_extract_parts "${{ env.S3_BUILD_STATE_ARCHIVE_BASENAME }}" "$RESTORE_TMP"
          echo "CloudFront缓存全部还原完毕于 $RESTORE_TMP"

      - name: 空间检查与关键目录检测
        run: |
          RESTORE_TMP=/workdir/cloudfront_cache_check_tmp
          echo "== RUNNER DISK USAGE (AFTER RESTORE) =="
          df -h
          echo "== /workdir USAGE (AFTER RESTORE) =="
          du -sh /workdir/* || true
          echo "== 缓存目录结构预览 =="
          find $RESTORE_TMP | head -100
          for d in dl staging_dir build_dir .ccache; do
            path="$RESTORE_TMP/$d"
            if [ ! -d "$path" ] || [ -z "$(ls -A $path 2>/dev/null)" ]; then
              echo "::error::[致命] CDN缓存目录 $d 未成功恢复或为空!"
              exit 2
            else
              echo "[OK] CDN缓存目录 $d 存在且非空。"
              du -sh "$path"
            fi
          done

      - name: 自动清理检查目录
        run: |
          rm -rf /workdir/cloudfront_cache_check_tmp
          echo "已自动清理本地检查目录"
